{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:46<00:00, 564530.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 44251.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:17<00:00, 251073.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 748276.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Download training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "#Download test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        FashionMNIST\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\u001b[1;31mLength:\u001b[0m      60000\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\yuan tao\\anaconda3\\envs\\tunglinenv\\lib\\site-packages\\torchvision\\datasets\\mnist.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
      "\n",
      "Args:\n",
      "    root (string): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
      "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
      "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
      "        otherwise from ``t10k-images-idx3-ubyte``.\n",
      "    download (bool, optional): If True, downloads the dataset from the internet and\n",
      "        puts it in root directory. If dataset is already downloaded, it is not\n",
      "        downloaded again.\n",
      "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "    target_transform (callable, optional): A function/transform that takes in the\n",
      "        target and transforms it."
     ]
    }
   ],
   "source": [
    "training_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        FashionMNIST\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\u001b[1;31mLength:\u001b[0m      10000\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\yuan tao\\anaconda3\\envs\\tunglinenv\\lib\\site-packages\\torchvision\\datasets\\mnist.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
      "\n",
      "Args:\n",
      "    root (string): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
      "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
      "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
      "        otherwise from ``t10k-images-idx3-ubyte``.\n",
      "    download (bool, optional): If True, downloads the dataset from the internet and\n",
      "        puts it in root directory. If dataset is already downloaded, it is not\n",
      "        downloaded again.\n",
      "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "    target_transform (callable, optional): A function/transform that takes in the\n",
      "        target and transforms it."
     ]
    }
   ],
   "source": [
    "test_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) and datatype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f'Shape of X[N, C, H, W]: {X.shape}')\n",
    "    print(f'Shape of y: {y.shape} and datatype: {y.dtype}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpu, mps, or cpu device for training\n",
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "print(f'Using {device} device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Defining model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer to train a model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m      \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mType:\u001b[0m           CrossEntropyLoss\n",
      "\u001b[1;31mString form:\u001b[0m    CrossEntropyLoss()\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\yuan tao\\anaconda3\\envs\\tunglinenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "This criterion computes the cross entropy loss between input logits\n",
      "and target.\n",
      "\n",
      "It is useful when training a classification problem with `C` classes.\n",
      "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
      "assigning weight to each of the classes.\n",
      "This is particularly useful when you have an unbalanced training set.\n",
      "\n",
      "The `input` is expected to contain the unnormalized logits for each class (which do `not` need\n",
      "to be positive or sum to 1, in general).\n",
      "`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n",
      ":math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
      "`K`-dimensional case. The last being useful for higher dimension inputs, such\n",
      "as computing cross entropy loss per-pixel for 2D images.\n",
      "\n",
      "The `target` that this criterion expects should contain either:\n",
      "\n",
      "- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n",
      "  `ignore_index` is specified, this loss also accepts this class index (this index\n",
      "  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
      "  set to ``'none'``) loss for this case can be described as:\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      "      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
      "      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
      "\n",
      "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = \\begin{cases}\n",
      "          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
      "           \\text{if reduction} = \\text{`mean';}\\\\\n",
      "            \\sum_{n=1}^N l_n,  &\n",
      "            \\text{if reduction} = \\text{`sum'.}\n",
      "        \\end{cases}\n",
      "\n",
      "  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n",
      "  on an input, followed by :class:`~torch.nn.NLLLoss`.\n",
      "\n",
      "- Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
      "  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
      "  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      "      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n",
      "\n",
      "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
      "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
      "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
      "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
      "\n",
      "  .. math::\n",
      "      \\ell(x, y) = \\begin{cases}\n",
      "          \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
      "           \\text{if reduction} = \\text{`mean';}\\\\\n",
      "            \\sum_{n=1}^N l_n,  &\n",
      "            \\text{if reduction} = \\text{`sum'.}\n",
      "        \\end{cases}\n",
      "\n",
      ".. note::\n",
      "    The performance of this criterion is generally better when `target` contains class\n",
      "    indices, as this allows for optimized computation. Consider providing `target` as\n",
      "    class probabilities only when a single class label per minibatch item is too restrictive.\n",
      "\n",
      "Args:\n",
      "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
      "        If given, has to be a Tensor of size `C` and floating point dtype\n",
      "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "        the losses are averaged over each loss element in the batch. Note that for\n",
      "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "        when :attr:`reduce` is ``False``. Default: ``True``\n",
      "    ignore_index (int, optional): Specifies a target value that is ignored\n",
      "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
      "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "        losses are averaged or summed over observations for each minibatch depending\n",
      "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
      "        be applied, ``'mean'``: the weighted mean of the output is taken,\n",
      "        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "        and :attr:`reduce` are in the process of being deprecated, and in\n",
      "        the meantime, specifying either of those two args will override\n",
      "        :attr:`reduction`. Default: ``'mean'``\n",
      "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      "        become a mixture of the original ground truth and a uniform distribution as described in\n",
      "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      "\n",
      "Shape:\n",
      "    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of `K`-dimensional loss.\n",
      "    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      "      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      "      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      "    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n",
      "\n",
      "\n",
      "    where:\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            C ={} & \\text{number of classes} \\\\\n",
      "            N ={} & \\text{batch size} \\\\\n",
      "        \\end{aligned}\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Example of target with class indices\n",
      "    >>> loss = nn.CrossEntropyLoss()\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
      "    >>> output = loss(input, target)\n",
      "    >>> output.backward()\n",
      "    >>>\n",
      "    >>> # Example of target with class probabilities\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      "    >>> output = loss(input, target)\n",
      "    >>> output.backward()\n",
      "\u001b[1;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "loss_fn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        SGD\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\yuan tao\\anaconda3\\envs\\tunglinenv\\lib\\site-packages\\torch\\optim\\sgd.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Implements stochastic gradient descent (optionally with momentum).\n",
      "\n",
      ".. math::\n",
      "   \\begin{aligned}\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      "            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      "        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      "        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      "        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      "        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      "        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      "        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
      "        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      "        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      "        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      "   \\end{aligned}\n",
      "\n",
      "Nesterov momentum is based on the formula from\n",
      "`On the importance of initialization and momentum in deep learning`__.\n",
      "\n",
      "Args:\n",
      "    params (iterable): iterable of parameters to optimize or dicts defining\n",
      "        parameter groups\n",
      "    lr (float, optional): learning rate (default: 1e-3)\n",
      "    momentum (float, optional): momentum factor (default: 0)\n",
      "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      "    dampening (float, optional): dampening for momentum (default: 0)\n",
      "    nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      "    maximize (bool, optional): maximize the objective with respect to the\n",
      "        params, instead of minimizing (default: False)\n",
      "    foreach (bool, optional): whether foreach implementation of optimizer\n",
      "        is used. If unspecified by the user (so foreach is None), we will try to use\n",
      "        foreach over the for-loop implementation on CUDA, since it is usually\n",
      "        significantly more performant. Note that the foreach implementation uses\n",
      "        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
      "        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
      "        parameters through the optimizer at a time or switch this flag to False (default: None)\n",
      "    differentiable (bool, optional): whether autograd should\n",
      "        occur through the optimizer step in training. Otherwise, the step()\n",
      "        function runs in a torch.no_grad() context. Setting to True can impair\n",
      "        performance, so leave it False if you don't intend to run autograd\n",
      "        through this instance (default: False)\n",
      "\n",
      "\n",
      "Example:\n",
      "    >>> # xdoctest: +SKIP\n",
      "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "    >>> optimizer.zero_grad()\n",
      "    >>> loss_fn(model(input), target).backward()\n",
      "    >>> optimizer.step()\n",
      "\n",
      "__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      "\n",
      ".. note::\n",
      "    The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      "    Sutskever et. al. and implementations in some other frameworks.\n",
      "\n",
      "    Considering the specific case of Momentum, the update can be written as\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      "        \\end{aligned}\n",
      "\n",
      "    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      "    parameters, gradient, velocity, and momentum respectively.\n",
      "\n",
      "    This is in contrast to Sutskever et. al. and\n",
      "    other frameworks which employ an update of the form\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      "            p_{t+1} & = p_{t} - v_{t+1}.\n",
      "        \\end{aligned}\n",
      "\n",
      "    The Nesterov version is analogously modified.\n",
      "\n",
      "    Moreover, the initial value of the momentum buffer is set to the\n",
      "    gradient value at the first step. This is in contrast to some other\n",
      "    frameworks that initialize it to all zeros."
     ]
    }
   ],
   "source": [
    "optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f'loss: {loss:>7f}[{current:>5d}/{size:>5d}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y.type(torch.float).sum().item())\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.782906[   64/60000]\n",
      "loss: 0.846840[ 6464/60000]\n",
      "loss: 0.577080[12864/60000]\n",
      "loss: 0.779403[19264/60000]\n",
      "loss: 0.669324[25664/60000]\n",
      "loss: 0.637760[32064/60000]\n",
      "loss: 0.719444[38464/60000]\n",
      "loss: 0.690457[44864/60000]\n",
      "loss: 0.709868[51264/60000]\n",
      "loss: 0.657622[57664/60000]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m----> 5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 10\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m      8\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m      9\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 10\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     11\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n\u001b[0;32m     12\u001b[0m correct \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m size\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
